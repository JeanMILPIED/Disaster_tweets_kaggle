{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step1: import the proper packages to the notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages we may need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random; random.seed(53)\n",
    "\n",
    "#import some specific NPL packages\n",
    "import nltk\n",
    "\n",
    "# Import all we need from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2: load the test and train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('./train.csv')\n",
    "data_test=pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3: descriptive exploration of the 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the main features\n",
    "def data_set_exploration(dataset):\n",
    "    print(dataset.shape)\n",
    "    print(dataset.columns)\n",
    "    print('\\n')\n",
    "    print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
      "\n",
      "\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_set_exploration(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 7613 tweets in the train database with 61 keyword missing and 2533 location missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 4)\n",
      "Index(['id', 'keyword', 'location', 'text'], dtype='object')\n",
      "\n",
      "\n",
      "id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_set_exploration(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 3263 tweets in the test database with 26 keyword missing and 1103 location missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the ratio of missing data in the 2 datasets are roughly the same: good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a function that fill missing values with mean value\n",
    "def my_fill_na_function(dataset):\n",
    "    for my_column in dataset.columns:\n",
    "        max_value= dataset[my_column].value_counts().index[0]\n",
    "        dataset[my_column]=dataset[my_column].fillna(max_value)\n",
    "        print('the max_value of column %s is %s' %(my_column,max_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the max_value of column id is 10235\n",
      "the max_value of column keyword is deluged\n",
      "the max_value of column location is New York\n",
      "the max_value of column text is 11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...\n"
     ]
    }
   ],
   "source": [
    "my_fill_na_function(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "keyword     0\n",
       "location    0\n",
       "text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the max_value of column id is 2047\n",
      "the max_value of column keyword is fatalities\n",
      "the max_value of column location is USA\n",
      "the max_value of column text is 11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...\n",
      "the max_value of column target is 0\n"
     ]
    }
   ],
   "source": [
    "my_fill_na_function(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a function that replaces the non alpha characters from the strings in the database befor next steps\n",
    "\n",
    "def my_remove_nonalpha(dataset):\n",
    "    for my_col in ['keyword','location','text']:\n",
    "        my_pattern=re.compile('[^A-Za-z]+')\n",
    "        dataset[my_col]=[my_pattern.sub(' ',my_text) for my_text in dataset[my_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_remove_nonalpha(data_train)\n",
    "my_remove_nonalpha(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10876, 5)\n",
      "Index(['id', 'keyword', 'location', 'target', 'text'], dtype='object')\n",
      "\n",
      "\n",
      "id             0\n",
      "keyword        0\n",
      "location       0\n",
      "target      3263\n",
      "text           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeanm\\Anaconda3\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# we concatenate horizonatally the 2 databases\n",
    "my_data_all=pd.concat([data_train,data_test], axis=0)\n",
    "data_set_exploration(my_data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4: start the NLP transformation of the database content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# we are going to tokenize and stem the 'keyword' columns of the test and train datasets merge in my_data_all\n",
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "keyword_split=[my_keyword.split() for my_keyword in my_data_all['keyword']]\n",
    "new_keyword_split=pd.DataFrame(keyword_split)\n",
    "\n",
    "my_row_max=new_keyword_split.shape[0]\n",
    "my_col_max=new_keyword_split.shape[1]\n",
    "my_count=0\n",
    "for my_col in range(my_col_max):\n",
    "    new_keyword_split.iloc[:,my_col].astype(str)\n",
    "    for my_row in range(my_row_max):\n",
    "        if new_keyword_split.iloc[my_row,my_col] is not None:\n",
    "            try: \n",
    "                new_keyword_split.iloc[my_row,my_col]=porter.stem(new_keyword_split.iloc[my_row,my_col])\n",
    "            except: \n",
    "                my_count+=1\n",
    "print(my_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatal       211\n",
       "scream      150\n",
       "suicid      150\n",
       "obliter     150\n",
       "emerg       150\n",
       "           ... \n",
       "war          35\n",
       "battl        33\n",
       "threat       16\n",
       "radiat       14\n",
       "epicentr     13\n",
       "Name: 0, Length: 159, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now have a reduced amount of keywords that have been stemmed and split into 3 columns\n",
    "new_keyword_split.iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatal           211\n",
       "flood           150\n",
       "hijack          150\n",
       "drown           150\n",
       "scream          150\n",
       "               ... \n",
       "war zone         35\n",
       "battl            33\n",
       "threat           16\n",
       "radiat emerg     14\n",
       "epicentr         13\n",
       "Name: final_kw, Length: 166, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we join the 3 columns in one sole column\n",
    "new_keyword_split['final_kw']=new_keyword_split.iloc[:,0:2].apply(lambda x: None if x.isnull().all() else ' '.join(x.dropna()), axis=1)\n",
    "\n",
    "# we only keep the first column\n",
    "my_new_keyword=new_keyword_split['final_kw']\n",
    "\n",
    "#how many individual items?: 166\n",
    "my_new_keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# we are going to tokenize and stem the 'text' columns of the test and train datasets merge in my_data_all\n",
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "text_split=[my_text.split() for my_text in my_data_all['text']]\n",
    "new_text_split=pd.DataFrame(text_split)\n",
    "\n",
    "my_row_max=new_text_split.shape[0]\n",
    "my_col_max=new_text_split.shape[1]\n",
    "my_count=0\n",
    "for my_col in range(my_col_max):\n",
    "    new_text_split.iloc[:,my_col].astype(str)\n",
    "    for my_row in range(my_row_max):\n",
    "        if new_text_split.iloc[my_row,my_col] is not None:\n",
    "            try: \n",
    "                new_text_split.iloc[my_row,my_col]=porter.stem(new_text_split.iloc[my_row,my_col])\n",
    "            except: \n",
    "                my_count+=1\n",
    "print(my_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 33)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we join all the columns in one sole column\n",
    "new_text_split['final_text']=new_text_split.apply(lambda x: None if x.isnull().all() else ' '.join(x.dropna()), axis=1)\n",
    "\n",
    "# we only keep the first column\n",
    "my_new_text=new_text_split['final_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    our deed are the reason of thi earthquak may a...\n",
       "1                 forest fire near La rong sask canada\n",
       "2    all resid ask to shelter in place are be notif...\n",
       "3       peopl receiv wildfir evacu order in california\n",
       "4    just got sent thi photo from rubi alaska as sm...\n",
       "Name: final_text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create the new dataframe\n",
    "my_data_all['new_kw']=my_new_keyword\n",
    "my_data_all['new_text']=my_new_text\n",
    "my_data_all=my_data_all.drop(['keyword','text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>target</th>\n",
       "      <th>new_kw</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>USA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fatal</td>\n",
       "      <td>our deed are the reason of thi earthquak may a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>USA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fatal</td>\n",
       "      <td>forest fire near La rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>USA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fatal</td>\n",
       "      <td>all resid ask to shelter in place are be notif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>USA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fatal</td>\n",
       "      <td>peopl receiv wildfir evacu order in california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>USA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fatal</td>\n",
       "      <td>just got sent thi photo from rubi alaska as sm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id location  target new_kw  \\\n",
       "0   1      USA     1.0  fatal   \n",
       "1   4      USA     1.0  fatal   \n",
       "2   5      USA     1.0  fatal   \n",
       "3   6      USA     1.0  fatal   \n",
       "4   7      USA     1.0  fatal   \n",
       "\n",
       "                                            new_text  \n",
       "0  our deed are the reason of thi earthquak may a...  \n",
       "1               forest fire near La rong sask canada  \n",
       "2  all resid ask to shelter in place are be notif...  \n",
       "3     peopl receiv wildfir evacu order in california  \n",
       "4  just got sent thi photo from rubi alaska as sm...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we save the tokenized and stemmed database\n",
    "my_data_all.to_csv('./my_data_all_stemmed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load back the database\n",
    "my_data_all_clean=pd.read_csv('./my_data_all_stemmed.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create back the train and test databases\n",
    "my_train_data_clean=my_data_all_clean.dropna(axis=0, subset=['target'])\n",
    "my_test_data_clean=my_data_all_clean[my_data_all_clean['target'].isna()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of stop words\n",
    "stoplist = set('for a of the and to in to be which some is at that we i who whom show via may my our might as well'.split())\n",
    "\n",
    "# we create a function that vectorizes the text of the targetted text data (fit_transform of train, transform of test)\n",
    "# and returns the concatenation of both\n",
    "def text_vectorizer(train_series, test_series):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, min_df=0.005, max_df=0.9)\n",
    "    return pd.concat([pd.DataFrame(tfidf_vectorizer.fit_transform(train_series).toarray()),pd.DataFrame(tfidf_vectorizer.transform(test_series).toarray())],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_keyword=text_vectorizer(my_train_data_clean['new_kw'], my_test_data_clean['new_kw'])\n",
    "tfidf_text=text_vectorizer(my_train_data_clean['new_text'], my_test_data_clean['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_column_kw=[]\n",
    "for my_kw_index in range(tfidf_keyword.shape[1]):\n",
    "    my_column_kw.append('kw_'+str(my_kw_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_column_text=[]\n",
    "for my_text_index in range(tfidf_text.shape[1]):\n",
    "    my_column_text.append('text_'+str(my_text_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_column=['id','target']+my_column_kw+my_column_text+['encoded_location']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we remove the text, location and keyword columns from the train and test databases and replace by the tfidf columns\n",
    "\n",
    "my_new_data_all = pd.concat([my_data_all_clean,tfidf_keyword], axis=1)\n",
    "my_new_data_all = pd.concat([my_new_data_all,tfidf_text], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add a lebellized location column\n",
    "my_new_data_all['encoded_location']=my_new_data_all['location'].astype('category').cat.codes\n",
    "# my_new_data_all=my_new_data_all.drop('keyword', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_data_all=my_new_data_all.drop(['new_kw','location','new_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we set the new columns to the my_new_data_all\n",
    "my_new_data_all.columns=my_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3413    2674\n",
       "2312    1214\n",
       "0        140\n",
       "3441      65\n",
       "1910      58\n",
       "        ... \n",
       "21         1\n",
       "2068       1\n",
       "4111       1\n",
       "13         1\n",
       "2043       1\n",
       "Name: encoded_location, Length: 4312, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_data_all['encoded_location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5: we train a classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we split the train database into some train and test points with split function\n",
    "my_new_train_data=my_new_data_all.dropna(axis=0, subset=['target'])\n",
    "my_y=my_new_train_data['target']\n",
    "my_X=my_new_train_data.drop(['id','target','encoded_location'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(my_X,my_y, random_state=49,test_size=.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Score:   0.774\n",
      "MultinomialNB Score:   0.761\n",
      "MultinomialNB Score:   0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeanm\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# then, we create a LinearSVM model\n",
    "tfidf_tweet_svc = LinearSVC(penalty='l2', dual=False, max_iter=10000, tol=0.001, C=0.1).fit(X_train,y_train)\n",
    "# we run predict on the X_test to get predictions\n",
    "tfidf_tweet_svc_pred = tfidf_tweet_svc.predict(X_test)\n",
    "# we Calculate accuracy using the metrics module\n",
    "tfidf_tweet_svc_score = metrics.accuracy_score(y_test,tfidf_tweet_svc_pred)\n",
    "print(\"LinearSVC Score:   %0.3f\" % tfidf_tweet_svc_score)\n",
    "\n",
    "# then, we create a NaiveBaies classifier model\n",
    "tfidf_tweet_nb = MultinomialNB(alpha=1).fit(X_train,y_train)\n",
    "# we run predict on the X_test to get predictions\n",
    "tfidf_tweet_nb_pred = tfidf_tweet_nb.predict(X_test)\n",
    "# we Calculate accuracy using the metrics module\n",
    "tfidf_tweet_nb_score = metrics.accuracy_score(y_test,tfidf_tweet_nb_pred)\n",
    "print(\"MultinomialNB Score:   %0.3f\" % tfidf_tweet_nb_score)\n",
    "\n",
    "# then, we create a logistic regression classifier model\n",
    "tfidf_tweet_lr = LogisticRegression().fit(X_train,y_train)\n",
    "# we run predict on the X_test to get predictions\n",
    "tfidf_tweet_lr_pred = tfidf_tweet_lr.predict(X_test)\n",
    "# we Calculate accuracy using the metrics module\n",
    "tfidf_tweet_lr_score = metrics.accuracy_score(y_test,tfidf_tweet_lr_pred)\n",
    "print(\"MultinomialNB Score:   %0.3f\" % tfidf_tweet_nb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC works best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6: we save the predicted results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeanm\\Anaconda3\\envs\\test\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "final_X_test=my_new_data_all[my_new_data_all['target'].isna()==True]\n",
    "my_X=final_X_test.drop(['id','target','encoded_location'], axis=1)\n",
    "final_X_test.loc[:,'target']=tfidf_tweet_svc.predict(my_X)\n",
    "final_X_test=final_X_test.loc[:,('id','target')]\n",
    "final_X_test['id']=final_X_test['id'].astype('int32')\n",
    "final_X_test=final_X_test.set_index('id')\n",
    "final_X_test['target']=final_X_test['target'].astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_X_test.to_csv('./NLP_submission_JM_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why does the new workflow gives so shitty predictions on the test sample while giving excellent train/test results on the train dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7: understanding the differences in predictions on the test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_2=pd.read_csv('./NLP_submission_JM_2.csv')\n",
    "sub_3=pd.read_csv('./NLP_submission_JM_3.csv')\n",
    "sub_4=pd.read_csv('./NLP_submission_JM_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2104\n",
      "1    1159\n",
      "Name: target, dtype: int64\n",
      "0    2117\n",
      "1    1146\n",
      "Name: target, dtype: int64\n",
      "0    2172\n",
      "1    1091\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sub_2['target'].value_counts())\n",
    "print(sub_3['target'].value_counts())\n",
    "print(sub_4['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
